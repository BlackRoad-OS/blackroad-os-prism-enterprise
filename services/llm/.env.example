# Service Configuration
LLM_HOST=0.0.0.0
LLM_PORT=8000
LLM_LOG_LEVEL=info
LLM_WORKERS=1

# Default Provider
LLM_DEFAULT_PROVIDER=echo

# OpenAI Configuration
# LLM_OPENAI_API_KEY=sk-...
# LLM_OPENAI_BASE_URL=https://api.openai.com/v1
# LLM_OPENAI_DEFAULT_MODEL=gpt-4o-mini
# LLM_OPENAI_TIMEOUT=60

# Anthropic Configuration
# LLM_ANTHROPIC_API_KEY=sk-ant-...
# LLM_ANTHROPIC_BASE_URL=https://api.anthropic.com
# LLM_ANTHROPIC_DEFAULT_MODEL=claude-sonnet-4-5-20250929
# LLM_ANTHROPIC_TIMEOUT=60

# Ollama Configuration
LLM_OLLAMA_BASE_URL=http://localhost:11434
LLM_OLLAMA_DEFAULT_MODEL=llama3.1
LLM_OLLAMA_TIMEOUT=120

# Caching
# LLM_REDIS_URL=redis://localhost:6379/0
LLM_CACHE_ENABLED=false
LLM_CACHE_TTL=3600

# Rate Limiting
LLM_RATE_LIMIT_ENABLED=true
LLM_RATE_LIMIT_REQUESTS_PER_MINUTE=60
LLM_RATE_LIMIT_TOKENS_PER_MINUTE=100000

# Observability
LLM_METRICS_ENABLED=true
LLM_METRICS_PORT=9090
LLM_TRACING_ENABLED=false
# LLM_TRACING_ENDPOINT=http://jaeger:14268/api/traces

# Request Defaults
LLM_DEFAULT_MAX_TOKENS=1024
LLM_DEFAULT_TEMPERATURE=0.7
LLM_DEFAULT_TOP_P=1.0

# Security
LLM_REQUIRE_AUTH=false
# LLM_API_KEYS=key1,key2,key3
